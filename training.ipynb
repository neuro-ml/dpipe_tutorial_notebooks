{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "# Training\n",
        "\n",
        "`deep_pipe` has a unified interface for training models. We will show an example for a model written in PyTorch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "from dpipe.torch import train_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "this is the main function; it requires a \\$TorchModel\\$, a \\$BatchIter\\$ , the learning rate and the number of epochs. \n",
        "\n",
        "Let\u0027s build all the required components."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### Model\n",
        "\n",
        "The model encapsulates all the logic necessary for training and inference with a given architecture:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "from dpipe.torch import TorchModel\n",
        "from dpipe import layers\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "architecture \u003d nn.Sequential(\n",
        "    nn.Conv2d(1, 32, kernel_size\u003d3),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(32, 64, kernel_size\u003d3),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(64, 128, kernel_size\u003d3),\n",
        "    \n",
        "    layers.PyramidPooling(nn.functional.max_pool2d), # global pooling\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 10),\n",
        ")\n",
        "\n",
        "model \u003d TorchModel(architecture, nn.Softmax(-1), nn.CrossEntropyLoss(), torch.optim.Adam)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "It requires the architecture itself, as well as the loss function, the activation layer, and the optimizer class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### Batch iterator\n",
        "\n",
        "The batch iterators are covered in a separate tutorial (:doc:\\$tutorials/batch_iter\\$), we\u0027ll reuse the code from it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": "from dpipe.batch_iter import Infinite\nfrom dpipe.tests.mnist.resources import MNIST\nfrom dpipe.batch_iter import load_by_random_id\n\ndataset \u003d MNIST(\u0027PATH TO DATA\u0027)\n\nbatch_iter \u003d Infinite(\n    load_by_random_id(dataset.load_image, dataset.load_label, ids\u003ddataset.ids),\n    batch_size\u003d100, batches_per_epoch\u003d2000,\n)"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### Training the model\n",
        "\n",
        "Next, we just run the function `train_model`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "train_model(model, batch_iter, n_epochs\u003d10, lr\u003d1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### Logging and Checkpoints\n",
        "\n",
        "After the calling `train_model` the interpreter just “hangs” until the training is over. In order to log various information about the training process, you can pass the `log_path` argument. The logs will be written in a format, readable by tensorboard.\n",
        "\n",
        "Also, it is often useful to keep checkpoints (or snapshots) of you model and optimizer in case you may want to resotore them. To do that, pass the `checkpoints_path` argument:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "train_model(model, batch_iter, n_epochs\u003d10, lr\u003d1e-3,\n",
        "            log_path\u003d\u0027~/my_new_model/logs\u0027, checkpoints_path\u003d\u0027~/my_new_model/checkpoints\u0027)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "The cool part is that if the training is prematurely stopped, e.g. by an exception, you can resume the training from the same point instead of starting over:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "train_model(model, batch_iter, n_epochs\u003d10, lr\u003d1e-3,\n",
        "            log_path\u003d\u0027~/my_new_model/logs\u0027, checkpoints_path\u003d\u0027~/my_new_model/checkpoints\u0027)\n",
        "# ... something bad happened, e.g. KeyboardInterrupt\n",
        "\n",
        "# start from where you left off\n",
        "train_model(model, batch_iter, n_epochs\u003d10, lr\u003d1e-3,\n",
        "            log_path\u003d\u0027~/my_new_model/logs\u0027, checkpoints_path\u003d\u0027~/my_new_model/checkpoints\u0027)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### Policies\n",
        "You can further customize the training process by passing addtitional policies.\n",
        "\n",
        "The \\$Policy\\$ interface has two methods:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "class Policy:\n",
        "    def epoch_started(self, epoch: int):\n",
        "        # ...\n",
        "\n",
        "    def epoch_finished(self, epoch: int, *, train_losses: Sequence \u003d None, metrics: dict \u003d None):\n",
        "        # ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "which are called at the start and end (respectively) of each epoch. The simplest setting in which they can be used is early stopping:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from dpipe.train.policy import EarlyStopping, Policy\n",
        "\n",
        "class TrainLossStop(Policy):\n",
        "    def epoch_finished(self, epoch: int, *, train_losses, metrics: dict \u003d None):\n",
        "        if np.mean(train_losses) \u003c 1e-5:\n",
        "            raise EarlyStopping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "this policy raises a specific exception - \\$EarlyStopping\\$, in order to trigger early stopping if the train loss becomes smaller than 1e-5."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "#### Policies with values\n",
        "Another useful kind of policies are the ones that carry a value which changes over time. For example - \\$Exponential\\$, which implements an exponentially changing policy.\n",
        "\n",
        "Their `value` attribute is passed to the model’s \\$do_train_step\\$ method as a keyword argument.\n",
        "\n",
        "Let’s write a dummy model in order to demonstrate this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "from dpipe.train.policy import Exponential\n",
        "\n",
        "\n",
        "class WeightedModel(TorchModel):\n",
        "    def do_train_step(self, *inputs, lr, weights):\n",
        "        print(\u0027I received these weights:\u0027, weights)\n",
        "        # do something with the weights ...\n",
        "        # perform a train step ...\n",
        "        loss \u003d super().do_train_step(*inputs, lr\u003dlr)\n",
        "        return loss\n",
        "    \n",
        "\n",
        "model \u003d WeightedModel(architecture, nn.Softmax(-1), nn.CrossEntropyLoss(), torch.optim.Adam)\n",
        "\n",
        "train_model(model, batch_iter, n_epochs\u003d10, lr\u003d1e-3, weights\u003dExponential(initial\u003d10, multiplier\u003d0.5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "I received these weights: 10.0\n",
        "I received these weights: 10.0\n",
        "...\n",
        "I received these weights: 5.0\n",
        "I received these weights: 5.0\n",
        "...\n",
        "I received these weights: 2.5\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### Validation\n",
        "Finally, you may want to evaluate your network on a separate validation set after each epoch. This is done by the `validate` argument. It expects a function that simply returns a dictionary with the calculated metrics, e.g.:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "def validate():\n",
        "    return {\n",
        "        \u0027acuracy\u0027: 0.95,\n",
        "        \u0027roc_auc\u0027: 0.91,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "this is a dummy function, of course."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Miniconda3",
      "language": "python",
      "name": "miniconda3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}