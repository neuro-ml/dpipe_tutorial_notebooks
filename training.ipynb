{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# Training\n",
    "\n",
    "`deep_pipe` has a unified interface for training models. We will show an example for a model written in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "from dpipe.train import train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "this is the main function; it requires a batch iterator, and a `train_step` function, that performs a forward-backward pass for a given batch.\n",
    "\n",
    "Let's build all the required components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### Batch iterator\n",
    "\n",
    "The batch iterators are covered in a separate tutorial (:doc:\\$batch_iter\\$), we'll reuse the code from it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "from dpipe.tests.mnist.resources import MNIST\n",
    "from dpipe.batch_iter import Infinite, load_by_random_id\n",
    "\n",
    "dataset = MNIST('PATH TO DATA')\n",
    "\n",
    "batch_iter = Infinite(\n",
    "    load_by_random_id(dataset.load_image, dataset.load_label, ids=dataset.ids),\n",
    "    batch_size=100, batches_per_epoch=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### Train Step\n",
    "\n",
    "Next, we will implement the function that performs a train_step. But first we need an architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from dpipe import layers\n",
    "\n",
    "\n",
    "architecture = nn.Sequential(\n",
    "    nn.Conv2d(1, 32, kernel_size=3),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(32, 64, kernel_size=3),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(64, 128, kernel_size=3),\n",
    "    \n",
    "    nn.AdaptiveMaxPool2d((1, 1)),\n",
    "    layers.Reshape('0', -1),\n",
    "    \n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 10),\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(architecture.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dpipe.torch import to_var, to_np\n",
    "\n",
    "def cls_train_step(images, labels):\n",
    "    # images and labels are both of type `np.ndarray`\n",
    "    images, labels = torch.from_numpy(images), torch.from_numpy(labels)\n",
    "    architecture.train()\n",
    "    \n",
    "    logits = architecture(images)\n",
    "    loss = criterion(logits, labels)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # `train_step` must return the loss which will be later user for logging\n",
    "    return loss.data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### Training the model\n",
    "\n",
    "Next, we just run the `train` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "train(cls_train_step, batch_iter, n_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more general version of the function `cls_train_step` is already available in dpipe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dpipe.torch import train_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from the input batches it requires the following arguments: `architecture`, `optimizer`, `criterion`. We can pass these arguments directly to `train`, so the previous call is equivalent to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    train_step, batch_iter, n_epochs=10, \n",
    "    architecture=architecture, optimizer=optimizer, criterion=criterion\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### Logging\n",
    "\n",
    "After calling `train` the interpreter just “hangs” until the training is over. In order to log various information about the training process, you can pass a logger: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00000: train loss: 0.29427966475486755\n",
      "00001: train loss: 0.26119616627693176\n",
      "00002: train loss: 0.2186189591884613\n"
     ]
    }
   ],
   "source": [
    "from dpipe.train import ConsoleLogger\n",
    "\n",
    "train(\n",
    "    train_step, batch_iter, n_epochs=3, logger=ConsoleLogger(),\n",
    "    architecture=architecture, optimizer=optimizer, criterion=criterion\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various logger implementations, e.g. one that writes in a format, readable by tensorboard - \\$TBLogger\\$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often useful to keep checkpoints (or snapshots) of you model and optimizer in case you may want to resotore them. To do that, pass the `checkpoint_manager` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "from dpipe.train import CheckpointManager\n",
    "\n",
    "\n",
    "checkpoints = CheckpointManager(\n",
    "    'PATH/TO/CHECKPOINTS/FOLDER', \n",
    "    {'model.pth': architecture, 'optimizer.pth': optimizer}\n",
    ")\n",
    "\n",
    "train(\n",
    "    train_step, batch_iter, n_epochs=3, checkpoint_manager=checkpoints,\n",
    "    architecture=architecture, optimizer=optimizer, criterion=criterion\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "The cool part is that if the training is prematurely stopped, e.g. by an exception, you can resume the training from the same point instead of starting over:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "train(\n",
    "    train_step, batch_iter, n_epochs=3, checkpoint_manager=checkpoints,\n",
    "    architecture=architecture, optimizer=optimizer, criterion=criterion\n",
    ")\n",
    "# ... something bad happened, e.g. KeyboardInterrupt\n",
    "\n",
    "# start from where you left off\n",
    "train(\n",
    "    train_step, batch_iter, n_epochs=3, checkpoint_manager=checkpoints,\n",
    "    architecture=architecture, optimizer=optimizer, criterion=criterion\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### Value Policies\n",
    "You can further customize the training process by passing addtitional values to `train_step` that change in time.\n",
    "\n",
    "For example, `train_step` takes an optional argument `lr` - used to update the `optimizer`'s learning rate. \n",
    "\n",
    "We can change this value after each trainig epoch using the \\$ValuePolicy\\$ interface. Let's use an exponential learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dpipe.train import Exponential\n",
    "\n",
    "train(\n",
    "    train_step, batch_iter, n_epochs=10, \n",
    "    architecture=architecture, optimizer=optimizer, criterion=criterion,\n",
    "    lr=Exponential(initial=1e-3, multiplier=0.5, step_length=3) # decrease by a factor of 2 every 3 epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### Validation\n",
    "Finally, you may want to evaluate your network on a separate validation set after each epoch. This is done by the `validate` argument. It expects a function that simply returns a dictionary with the calculated metrics, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "def validate():\n",
    "    return {\n",
    "        'acuracy': 0.95,\n",
    "        'roc_auc': 0.91,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "this is a dummy function, of course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    train_step, batch_iter, n_epochs=10, validate=validate,\n",
    "    architecture=architecture, optimizer=optimizer, criterion=criterion,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Miniconda3",
   "language": "python",
   "name": "miniconda3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
